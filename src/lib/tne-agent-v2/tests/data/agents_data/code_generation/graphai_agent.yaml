agent: nestedAgent
params:
  useUserQuery: false
graph:
  version: 0.5
  nodes:
    n3-LF1YWYsHeXmUQ9UQOp_codeGeneration:
      agent: codeGenerationTemplateAgent
      params:
        prompt: "123"
      passThrough:
        nodeType: codeGeneration_template
        nodeId: n3-LF1YWYsHeXmUQ9UQOp_codeGeneration
        nodeTitle: Generate Code
      inputs:
        file: ":file"
        inputs: ":inputs"
        userPrompt: ":userPrompt"
    n3-LF1YWYsHeXmUQ9UQOp_llm:
      agent: openAIAgent
      params:
        model: gpt-4o
        max_tokens: 500
        temperature: 0
        mergeableSystem: "123"
        stream: false
        useUserQuery: false
      passThrough:
        nodeType: llm
        nodeTitle: New Model
        nodeId: n3-LF1YWYsHeXmUQ9UQOp_llm
        llmType: openAI
      isResult: true
      inputs:
        prompt: ":n3-LF1YWYsHeXmUQ9UQOp_codeGeneration.prompt"
        model: ":n3-LF1YWYsHeXmUQ9UQOp_codeGeneration.model"
        system: ":n3-LF1YWYsHeXmUQ9UQOp_codeGeneration.system"
        temperature: ":n3-LF1YWYsHeXmUQ9UQOp_codeGeneration.temperature"
        max_tokens: ":n3-LF1YWYsHeXmUQ9UQOp_codeGeneration.max_tokens"
    n3-LF1YWYsHeXmUQ9UQOp_python:
      agent: pythonCodeAgent
      params: {}
      passThrough:
        nodeType: python_code
        nodeId: n3-LF1YWYsHeXmUQ9UQOp_python
        nodeTitle: Generate Code
      isResult: true
      inputs:
        code: ":n3-LF1YWYsHeXmUQ9UQOp_llm.choices.$0.message.content"
passThrough:
  nodeType: code_generation
  nodeId: n3-LF1YWYsHeXmUQ9UQOp
  nodeTitle: Generate Code
isResult: false
